{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Research\n",
    "Till a few years ago, in the days before Deep Learning, machine learning applications of Computer Vision used to rely on traditional image processing techniques to do feature engineering. For instance, we would generate hand-crafted features using algorithms to detect corners, edges, and faces. With NLP applications as well, we would rely on techniques such as extracting N-grams and computing Term Frequency.\n",
    "\n",
    "What is more interesting is that, with deep learning, we donâ€™t actually deal with audio data in its raw form. Instead, the common approach used is to convert the audio data into images and then use a standard CNN architecture to process those images! Really? Convert sound into pictures? That sounds like science fiction. ðŸ˜„\n",
    "\n",
    "A Spectrogram of a signal plots its Spectrum over time and is like a â€˜photographâ€™ of the signal. It plots Time on the x-axis and Frequency on the y-axis. It is as though we took the Spectrum again and again at different instances in time, and then joined them all together into a single plot.\n",
    "\n",
    "It uses different colors to indicate the Amplitude or strength of each frequency. The brighter the color the higher the energy of the signal. Each vertical â€˜sliceâ€™ of the Spectrogram is essentially the Spectrum of the signal at that instant in time and shows how the signal strength is distributed in every frequency found in the signal at that instant.\n",
    "\n",
    "Optionally, use simple audio processing techniques to augment the spectrogram data. (Some augmentation or cleaning can also be done on the raw audio data before the spectrogram conversion)\n",
    "Now that we have image data, we can use standard CNN architectures to process them and extract feature maps that are an encoded representation of the spectrogram image.    \n",
    "\n",
    "For instance, for an audio classification problem, you would pass this through a Classifier usually consisting of some fully connected Linear layers.\n",
    "For a Speech-to-Text problem, you could pass it through some RNN layers to extract text sentences from this encoded representation.\n",
    "\n",
    "With the popularity of music streaming services, another common application that most of us are familiar with is to identify and categorize music based on the audio. The content of the music is analyzed to figure out the genre to which it belongs. This is a multi-label classification problem because a given piece of music might fall under more than one genre. eg. rock, pop, jazz, salsa, instrumental as well as other facets such as â€˜oldiesâ€™, â€˜female vocalistâ€™, â€˜happyâ€™, â€˜party musicâ€™ and so on.\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ai_ml_nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75f149aabefc45f596bad3ecd1cce999665dc47c609d2b6ed1c0c06088326543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
